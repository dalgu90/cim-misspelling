{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC-III Misspelling Synthetic Dataset (with new lexicon)\n",
    "\n",
    "This notebook makes an synthetic typo dataset from MIMIC-III dataset. We randomly choose words from clinical notes, corrupt them.\n",
    "\n",
    "The possible corruptions are (We do one or two corruptions for each word):\n",
    "1. Adding a character\n",
    "2. Deleting a character\n",
    "3. Substituting a character\n",
    "4. Swaping two adjacent characters\n",
    "\n",
    "The dictionary (a set of valid words) generated with the `LRWD` and the `prevariants` table of UMLS and a English dictionary from [here](https://github.com/dwyl/english-words).\n",
    "\n",
    "The output of this notebook is a dataset of (context, typo, answer) in TSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "import random\n",
    "import multiprocessing\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from utils import clean_text, sanitize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "mimic_note_fpath = '../data/mimic3/NOTEEVENTS.csv'  # MIMIC-III\n",
    "mimic_tools_dpath = '../scripts/mimic-tools/'  # Pseudonymization\n",
    "lexicon_fpath = '../data/lexicon/lexicon.json'  # Dictionary\n",
    "\n",
    "# Output path\n",
    "data_root = '../data/mimic_synthetic/'\n",
    "num_val_examples = 10000\n",
    "num_test_examples = 10000\n",
    "num_examples = num_val_examples + num_test_examples\n",
    "all_output_fpath = os.path.join(data_root, 'all.tsv')\n",
    "val_output_fpath = os.path.join(data_root, 'val.tsv')\n",
    "test_output_fpath = os.path.join(data_root, 'test.tsv')\n",
    "# min_word_len = 3\n",
    "min_word_len = 1\n",
    "no_corruption_prob = 0.1\n",
    "max_corruptions = 2\n",
    "do_substitution = True\n",
    "do_transposition = True\n",
    "DEFAULT_MAX_CHARACTER_POSITIONS = 64\n",
    "\n",
    "pseudo_in_dpath = os.path.join(data_root, 'temp')\n",
    "pseudo_out_dpath = os.path.join(data_root, 'temp_pseudonym')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read NOTEEVENTS.csv... done! 2083180 notes\n"
     ]
    }
   ],
   "source": [
    "# Read MIMIC-III notes\n",
    "print(f'Read {os.path.basename(mimic_note_fpath)}... ', end='')\n",
    "df_notes = pd.read_csv(mimic_note_fpath, low_memory=False)\n",
    "df_notes = df_notes.set_index('ROW_ID')\n",
    "print(f'done! {len(df_notes)} notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read ../data/lexicon/lexicon.json... 822919 words\n"
     ]
    }
   ],
   "source": [
    "# Load & preprocess clinspell lexicon\n",
    "print(f'Read {lexicon_fpath}... ', end='')\n",
    "with open(lexicon_fpath, 'r') as fd:\n",
    "    vocab = json.load(fd)\n",
    "vocab_set = set(vocab)\n",
    "print(f'{len(vocab)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select notes & corrupt words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose words\n",
    "puncs = list(\"[]!\\\"#$%&'()*+,./:;<=>?@\\^_`{|}~-\")\n",
    "\n",
    "def random_word_context(text, max_trial=100):\n",
    "    words = text.split()\n",
    "    \n",
    "    trial = 0\n",
    "    done = False\n",
    "    while trial < max_trial and not done:\n",
    "        # Select a word\n",
    "        trial += 1\n",
    "        w_idx = random.randint(0, len(words)-1)\n",
    "        word, left_res, right_res = words[w_idx], [], []\n",
    "        \n",
    "        # If the word is already in vocab, it's good to go.\n",
    "        if len(word) >= min_word_len and (word.lower() in vocab_set) and \\\n",
    "                len(word) < DEFAULT_MAX_CHARACTER_POSITIONS - 4:\n",
    "            done = True\n",
    "        else:\n",
    "            # Otherwise, detach puncs at the first and the last char, and check again\n",
    "            if word[0] in puncs:\n",
    "                word, left_res = word[1:], [word[0]]\n",
    "            else:\n",
    "                word, left_res = word, []\n",
    "            if not word: continue  # The word was just a punc\n",
    "\n",
    "            if word[-1] in puncs:\n",
    "                word, right_res = word[:-1], [word[-1]]\n",
    "            else:\n",
    "                word, right_res = word, []\n",
    "\n",
    "            if len(word) < min_word_len or (not word.lower() in vocab_set) or \\\n",
    "                    len(word) >= DEFAULT_MAX_CHARACTER_POSITIONS - 4:\n",
    "                continue\n",
    "\n",
    "            # Check whether it's anonymized field\n",
    "            right_snip = ' '.join(words[w_idx+1:w_idx+5])\n",
    "            if '**]' in right_snip and '[**' not in right_snip:\n",
    "                continue\n",
    "            left_snip = ' '.join(words[w_idx-4:w_idx])\n",
    "            if '[**' in left_snip and '**]' not in left_snip:\n",
    "                continue\n",
    "            \n",
    "            # Pass!\n",
    "            done = True\n",
    "            \n",
    "    if done:\n",
    "        return word, ' '.join(words[:w_idx] + left_res), ' '.join(right_res + words[w_idx+1:])\n",
    "    else:\n",
    "        raise ValueError('failed to choose word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrupt words\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def random_alphabet():\n",
    "    return random.choice(alphabet)\n",
    "\n",
    "operation_list = ['ins', 'del']\n",
    "if do_substitution:\n",
    "    operation_list.append('sub')\n",
    "if do_transposition:\n",
    "    operation_list.append('tra')\n",
    "\n",
    "def single_corruption(word):\n",
    "    while True:\n",
    "        oper = random.choice(operation_list)\n",
    "\n",
    "        if oper == \"del\":  # deletion\n",
    "            if len(word) == 1: continue\n",
    "            cidx = random.randint(0, len(word)-1)\n",
    "            ret = word[:cidx] + word[cidx+1:]\n",
    "            break\n",
    "        elif oper == \"ins\":  # insertion\n",
    "            cidx = random.randint(0, len(word))\n",
    "            ret = word[:cidx] + random_alphabet() + word[cidx:]\n",
    "            break\n",
    "        elif oper == \"sub\":  # substitution\n",
    "            cidx = random.randint(0, len(word)-1)\n",
    "            while True:\n",
    "                c = random_alphabet()\n",
    "                if c != word[cidx]:\n",
    "                    ret = word[:cidx] + c + word[cidx+1:]\n",
    "                    break\n",
    "        elif oper == \"tra\":  # transposition\n",
    "            if len(word) == 1 : continue\n",
    "            cidx = random.randint(0, len(word)-2) # swap cidx-th and (cidx+1)-th char\n",
    "            if word[cidx+1] == word[cidx]: continue\n",
    "            ret = word[:cidx] + word[cidx+1] + word[cidx] + word[cidx+2:]\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError(f'Wrong operation {oper}')\n",
    "    return ret\n",
    "\n",
    "def corrupt_word(word_original, max_corruptions=2):\n",
    "    if no_corruption_prob > 0.0:\n",
    "        if random.uniform(0, 1) < no_corruption_prob:\n",
    "            return word_original\n",
    "\n",
    "    num_corruption = random.randint(1, max_corruptions)\n",
    "    while True:\n",
    "        word = word_original\n",
    "        for i in range(num_corruption):\n",
    "            word = single_corruption(word)\n",
    "        if word_original != word:\n",
    "            break\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[655360, 393217, 524290, 393216, 393224, 16, 393244, 1572897, 34, 786477]\n"
     ]
    }
   ],
   "source": [
    "# Select note indexes randomly\n",
    "random.seed(1234)\n",
    "note_ids = list(df_notes.index)\n",
    "random.shuffle(note_ids)\n",
    "\n",
    "count, typo_noteids = 0, set()\n",
    "for nid in note_ids:\n",
    "    note = df_notes.loc[nid].TEXT\n",
    "    if len(note.strip()) >= 2000 and nid not in typo_noteids:  # Only choose for len(text) >= 1000\n",
    "        typo_noteids.add(nid)\n",
    "        count += 1\n",
    "    if count == num_examples:\n",
    "        break\n",
    "\n",
    "typo_noteids = list(typo_noteids)\n",
    "print(typo_noteids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3928.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Select words\n",
    "examples = []\n",
    "for nid in tqdm(typo_noteids):\n",
    "    text = df_notes.loc[nid].TEXT\n",
    "    word, left, right = random_word_context(text)\n",
    "    examples.append([word, left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 words have punctuation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['trans-jugular',\n",
       " \"patient's\",\n",
       " \"CX'S\",\n",
       " 'vancomycin-resistant',\n",
       " 'intra-',\n",
       " \"CK's\",\n",
       " \"MAP'S\",\n",
       " 'fat-containing',\n",
       " \"patient's\",\n",
       " \"Gerota's\",\n",
       " 'non-distended',\n",
       " \"patient's\",\n",
       " 'Post-hemorrhagic',\n",
       " \"patient's\",\n",
       " 'NON-CORONARY',\n",
       " 'sub-mandibular',\n",
       " 'G-tube',\n",
       " 'e-mail',\n",
       " 'MP-RAGE',\n",
       " 'non-heparin',\n",
       " 'IN-',\n",
       " \"rec'd\",\n",
       " 'Non-tender',\n",
       " 'post-surgical',\n",
       " 'c-collar',\n",
       " 'well-defined',\n",
       " 'ad-lib',\n",
       " 'lima-lad',\n",
       " \"patient's\",\n",
       " 't-cell',\n",
       " 'Non-tender',\n",
       " 'C-SPINE',\n",
       " 'time-out',\n",
       " 'post-op',\n",
       " 'wall-to-wall',\n",
       " 'Ill-defined',\n",
       " 'post-operative',\n",
       " 'Post-op',\n",
       " \"patient's\",\n",
       " 'c-pap',\n",
       " 're-intubate',\n",
       " \"patient's\",\n",
       " 'intra-abdominal',\n",
       " 'right-sided',\n",
       " 'moderate-sized',\n",
       " 'non-distended',\n",
       " 'x-ray',\n",
       " 'Non-tender',\n",
       " 're-oriented',\n",
       " 'intra-abdominal',\n",
       " 'double-lumen',\n",
       " \"Sat's\",\n",
       " 'work-up',\n",
       " 'Non-distended',\n",
       " \"non-Hodgkin's\",\n",
       " 'Non-tender',\n",
       " \"Patient's\",\n",
       " 'year-old',\n",
       " 'Non-invasive',\n",
       " 'mild-to-moderate',\n",
       " 'vaso-vagal',\n",
       " 'mild-to-moderate',\n",
       " 'extra-axial',\n",
       " \"family's\",\n",
       " \"Pt's\",\n",
       " 'third-order',\n",
       " 'LIMA-LAD',\n",
       " 'Non-tender',\n",
       " 'NON-CORONARY',\n",
       " 'follow-up',\n",
       " 'V-tach',\n",
       " 'MP-RAGE',\n",
       " 'post-pyloric',\n",
       " 'Well-defined',\n",
       " 'above-mentioned',\n",
       " 'peel-away',\n",
       " 'post-bypass',\n",
       " 'G-tube',\n",
       " 'Non-invasive',\n",
       " 'a-line',\n",
       " 'anti-GBM',\n",
       " 'Piperacillin-Tazobactam',\n",
       " \"rec'd\",\n",
       " 'post-operative',\n",
       " 'post-op',\n",
       " 'low-attenuation',\n",
       " 'intra-abdominal',\n",
       " 'a-line',\n",
       " 'post-bypass',\n",
       " 'retro-auricular',\n",
       " 'post-radiation',\n",
       " \"PRBC's\",\n",
       " 'Non-tender',\n",
       " 'x-ray',\n",
       " 'post-contrast',\n",
       " \"patient's\",\n",
       " 'non-progressive',\n",
       " 'pre-renal',\n",
       " 'A-fib',\n",
       " 'year-old',\n",
       " 'co-registered',\n",
       " 'a-line',\n",
       " 'mild-to-moderate',\n",
       " 'post-procedural',\n",
       " 'J-tube',\n",
       " 'a-line',\n",
       " 'Follow-up',\n",
       " 'a-line',\n",
       " \"pt's\",\n",
       " 'Non-tender',\n",
       " \"patient's\",\n",
       " 'A-Line',\n",
       " 'Swan-Ganz',\n",
       " 'Non-distended',\n",
       " \"MD'S\",\n",
       " 'single-lumen',\n",
       " 'aorto-bifemoral',\n",
       " 'Pre-procedure',\n",
       " \"patient's\",\n",
       " 'well-controlled',\n",
       " 'Non-distended',\n",
       " 'extra-hepatic',\n",
       " \"mother's\",\n",
       " \"Alzheimer's\",\n",
       " 'culture-negative',\n",
       " 'Endo-',\n",
       " 're-intubated',\n",
       " 'x-ray',\n",
       " 'ground-glass']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many words have punctuations\n",
    "words = list(zip(*examples))[0]\n",
    "words_with_punc = list(filter(lambda w: sum([not c.isalpha() for c in w]), words))\n",
    "print(f'{len(words_with_punc)} words have punctuation')\n",
    "words_with_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write notes with misspells\n",
    "if os.path.exists(pseudo_in_dpath):\n",
    "    shutil.rmtree(pseudo_in_dpath)        \n",
    "if os.path.exists(pseudo_out_dpath):\n",
    "    shutil.rmtree(pseudo_out_dpath)\n",
    "    \n",
    "os.makedirs(pseudo_in_dpath)\n",
    "for noteid, (_, left, right) in zip(typo_noteids, examples):\n",
    "    with open(os.path.join(pseudo_in_dpath, f'{noteid}_left.txt'), 'w', encoding='utf-8') as fd:\n",
    "        fd.write(left)\n",
    "    with open(os.path.join(pseudo_in_dpath, f'{noteid}_right.txt'), 'w', encoding='utf-8') as fd:\n",
    "        fd.write(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 20:02:38,532 Starting placeholder replacing\n",
      "2022-04-21 20:02:38,533 Loading lists\n",
      "2022-04-21 20:02:38,557 * Postal addresses: 20000 [656C Newport Court Coatesville, PA 19320 ...]\n",
      "2022-04-21 20:02:38,803 * Last names: 88799 [SMITH, JOHNSON, WILLIAMS, JONES, BROWN ...]\n",
      "2022-04-21 20:02:38,806 * Male first names: 1219 [JAMES, JOHN, ROBERT, MICHAEL, WILLIAM ...]\n",
      "2022-04-21 20:02:38,816 * Female first names: 4275 [MARY, PATRICIA, LINDA, BARBARA, ELIZABETH ...]\n",
      "2022-04-21 20:02:38,848 * Phone numbers: 20000 [(666) 372-7835, (923) 739-2644 ...]\n",
      "2022-04-21 20:02:38,874 * Companies: 20000 [Ligula Aenean Gravida Ltd, Non Bibendum Sed LLC ...]\n",
      "2022-04-21 20:02:38,874 * Countries: 264 [Afghanistan, Albania, Algeria, American Samoa ...]\n",
      "2022-04-21 20:02:38,896 * Emails: 20000 [enim.Suspendisse.aliquet@Crasdictum.com, sapien.Cras.dolor@Curabitur.org ...]\n",
      "2022-04-21 20:02:38,897 * Holiday names: 187 [Administrative Professionals Day, Air Force Birthday ...]\n",
      "2022-04-21 20:02:38,901 * Hospital names: 4805 [Southeast Alabama Medical Center, Marshall Medical Center South ...]\n",
      "2022-04-21 20:02:38,929 * Location names: 20000 [Prenzlau, Siegendorf, Hulste ...]\n",
      "2022-04-21 20:02:38,949 * SSN: 20000 [584-80-0392, 121-03-0002, 113-93-0738 ...]\n",
      "2022-04-21 20:02:38,949 * US_States: 50 [Alabama, Alaska, Arizona ...]\n",
      "2022-04-21 20:02:38,950 * Colleges: 450 [Adelphi University, Agnes Scott College, University of Alabama ...]\n",
      "2022-04-21 20:02:38,950 * Wards & Units: 29 [A & E, ICU, Operating Theatre ...]\n",
      "2022-04-21 20:02:38,951 * Websites: 500 [twitpic.com, wsj.com, wikimedia.org ...]\n",
      "2022-04-21 20:02:38,951 * Combining female and male first names: 5494 [MARY, PATRICIA, LINDA ...]\n",
      "2022-04-21 20:02:38,951 Creating mapper\n",
      "2022-04-21 20:02:38,951 Computing number of files to process\n",
      "2022-04-21 20:02:39,040 Replacing placeholders. This can take a long time...\n",
      "2022-04-21 20:02:39,494 Processed: 1000/40000 (2.5%)\n",
      "2022-04-21 20:02:39,924 Processed: 2000/40000 (5.0%)\n",
      "2022-04-21 20:02:40,314 Processed: 3000/40000 (7.5%)\n",
      "2022-04-21 20:02:40,742 Processed: 4000/40000 (10.0%)\n",
      "2022-04-21 20:02:41,225 Processed: 5000/40000 (12.5%)\n",
      "2022-04-21 20:02:41,623 Processed: 6000/40000 (15.0%)\n",
      "2022-04-21 20:02:42,037 Processed: 7000/40000 (17.5%)\n",
      "2022-04-21 20:02:42,500 Processed: 8000/40000 (20.0%)\n",
      "2022-04-21 20:02:42,892 Processed: 9000/40000 (22.5%)\n",
      "2022-04-21 20:02:43,632 Processed: 10000/40000 (25.0%)\n",
      "2022-04-21 20:02:45,281 Processed: 11000/40000 (27.5%)\n",
      "2022-04-21 20:02:46,289 Processed: 12000/40000 (30.0%)\n",
      "2022-04-21 20:02:47,363 Processed: 13000/40000 (32.5%)\n",
      "2022-04-21 20:02:48,492 Processed: 14000/40000 (35.0%)\n",
      "2022-04-21 20:02:49,602 Processed: 15000/40000 (37.5%)\n",
      "2022-04-21 20:02:50,679 Processed: 16000/40000 (40.0%)\n",
      "2022-04-21 20:02:52,264 Processed: 17000/40000 (42.5%)\n",
      "2022-04-21 20:02:53,677 Processed: 18000/40000 (45.0%)\n",
      "2022-04-21 20:02:54,766 Processed: 19000/40000 (47.5%)\n",
      "2022-04-21 20:02:55,847 Processed: 20000/40000 (50.0%)\n",
      "2022-04-21 20:02:56,858 Processed: 21000/40000 (52.5%)\n",
      "2022-04-21 20:02:57,886 Processed: 22000/40000 (55.0%)\n",
      "2022-04-21 20:02:58,876 Processed: 23000/40000 (57.5%)\n",
      "2022-04-21 20:02:59,958 Processed: 24000/40000 (60.0%)\n",
      "2022-04-21 20:03:00,883 Processed: 25000/40000 (62.5%)\n",
      "2022-04-21 20:03:01,779 Processed: 26000/40000 (65.0%)\n",
      "2022-04-21 20:03:02,567 Processed: 27000/40000 (67.5%)\n",
      "2022-04-21 20:03:03,360 Processed: 28000/40000 (70.0%)\n",
      "2022-04-21 20:03:04,145 Processed: 29000/40000 (72.5%)\n",
      "2022-04-21 20:03:04,914 Processed: 30000/40000 (75.0%)\n",
      "2022-04-21 20:03:05,704 Processed: 31000/40000 (77.5%)\n",
      "2022-04-21 20:03:06,452 Processed: 32000/40000 (80.0%)\n",
      "2022-04-21 20:03:07,261 Processed: 33000/40000 (82.5%)\n",
      "2022-04-21 20:03:08,032 Processed: 34000/40000 (85.0%)\n",
      "2022-04-21 20:03:08,393 Processed: 35000/40000 (87.5%)\n",
      "2022-04-21 20:03:08,745 Processed: 36000/40000 (90.0%)\n",
      "2022-04-21 20:03:09,085 Processed: 37000/40000 (92.5%)\n",
      "2022-04-21 20:03:09,444 Processed: 38000/40000 (95.0%)\n",
      "2022-04-21 20:03:09,785 Processed: 39000/40000 (97.5%)\n",
      "2022-04-21 20:03:10,158 Processed: 40000/40000 (100.0%)\n",
      "2022-04-21 20:03:10,159 Done !\n",
      "2022-04-21 20:03:10,171 Done ! (Time elapsed: 0:00:32)\n"
     ]
    }
   ],
   "source": [
    "# pip install requests joblib sqlalchemy gensim\n",
    "! python {os.path.join(mimic_tools_dpath, 'main.py')} REPLACE \\\n",
    "    --input-dir {os.path.join(os.getcwd(), pseudo_in_dpath)} \\\n",
    "    --output-dir {os.path.join(os.getcwd(), pseudo_out_dpath)} \\\n",
    "    --list-dir {os.path.join(mimic_tools_dpath, 'lists')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20000/20000 [00:39<00:00, 506.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read pseudonymized notes\n",
    "def process_note(note):\n",
    "    note = re.sub('\\n', ' ', note)\n",
    "    note = re.sub('\\t', ' ', note)\n",
    "    note = sanitize_text(clean_text(note))\n",
    "    return note\n",
    "\n",
    "for nid, example in tqdm(zip(typo_noteids, examples), total=len(typo_noteids)):\n",
    "    with open(os.path.join(pseudo_out_dpath, f'{nid}_left.txt'), 'r', encoding='utf-8') as fd:\n",
    "        note = fd.read()\n",
    "        note = process_note(note)\n",
    "        example[1] = note\n",
    "    with open(os.path.join(pseudo_out_dpath, f'{nid}_right.txt'), 'r', encoding='utf-8') as fd:\n",
    "        note = fd.read()\n",
    "        note = process_note(note)\n",
    "        example[2] = note\n",
    "    example[0] = example[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate corrupted words... \n",
      "\tbp -> dcbp\n",
      "\ttracking -> ztracking\n",
      "\tnot -> not\n",
      "\tmuch -> uwmch\n",
      "\tto -> ot\n",
      "\tpatient -> patient\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Corrupt words\n",
    "print('Generate corrupted words... ')\n",
    "random.seed(1234)\n",
    "correct_words = [e[0] for e in examples]\n",
    "typo_words = [corrupt_word(w, max_corruptions) for w in correct_words]\n",
    "for i, (w1, w2) in enumerate(zip(correct_words, typo_words)):\n",
    "    print(f'\\t{w1} -> {w2}')\n",
    "    if i == 5: break\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format (TSV): `index`,`note_id`,`word`,`left`,`right`,`correct`\n",
    "\n",
    "- `index`: index of the data (starting from 0)\n",
    "- `note_id`: ROW_ID of MIMIC-III `NOTEEVENTS.csv`\n",
    "- `word`: the word of interest (typo)\n",
    "- `left`: left context\n",
    "- `right`: right context\n",
    "- `correct`: correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "data_split_idx = list(range(num_examples))\n",
    "random.shuffle(data_split_idx)\n",
    "val_idx, test_idx = data_split_idx[:num_val_examples], data_split_idx[num_val_examples:]\n",
    "val_idx.sort()\n",
    "test_idx.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write examples to ../data/mimic_synthetic/all.tsv... done!\n",
      "Write examples to ../data/mimic_synthetic/val.tsv... done!\n",
      "Write examples to ../data/mimic_synthetic/test.tsv... done!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(data_root):\n",
    "    os.makedirs(data_root)\n",
    "\n",
    "for fpath, idx_list in [(all_output_fpath, list(range(num_examples))),\n",
    "                   (val_output_fpath, val_idx),\n",
    "                   (test_output_fpath, test_idx)]:\n",
    "    print(f'Write examples to {fpath}... ', end='', flush=True)\n",
    "    with open(fpath, 'w') as fd:\n",
    "        writer = csv.writer(fd, delimiter='\\t')\n",
    "        writer.writerow(['index', 'note_id', 'word', 'left', 'right', 'correct'])  \n",
    "        for i in idx_list:\n",
    "            nid, (correct, left, right), typo = typo_noteids[i], examples[i], typo_words[i]\n",
    "            left = ' '.join(left.split(' ')[-128:])\n",
    "            right = ' '.join(right.split(' ')[:128])\n",
    "            line = [i, nid, typo, left, right, correct]\n",
    "            writer.writerow(line)\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
